{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e2cc1c-8b6c-4b3c-841c-f94b29a7f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\锤子\\_netrc\n",
      "wandb: Currently logged in as: liumingyu720 (liumingyu720-huazhong-university-of-science-and-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n记录：\\n1.第一个5000轮，loss没有收敛，render看了一下训练成果，发现agent会卡在右上角不动，推测可能是训练轮数太少了，\\n到后面epsilon又变成0.1，导致探索性不足，q值趋于不好的稳定。\\n2.第二次，使用了上面第4000轮的权重，调整了一下epsilon从0.75-0.1，调整了一下lr=1.5e-4，但是loss发散了，发散的很严重，\\n  赶紧停掉了，重新设计一下参数。\\n3.第三次：\\nlr=0.9e-4， epsilon_start, epsilon_end, epsilon_decay = 0.7, 0.1, 6000000， num_episodes = 5000\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import wandb\n",
    "wandb.login(key=\"390acfbb12bfaf0cc52b7a946e4db99a58ed88f3\")\n",
    "\n",
    "\"\"\" test code \"\"\"\n",
    "# env = gym.make(\"ALE/Alien-v5\", render_mode=\"human\")\n",
    "# observation, info = env.reset()\n",
    "\n",
    "# for _ in range(500):\n",
    "#     action = env.action_space.sample()  # 随机动作\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "#     if terminated or truncated:\n",
    "#         observation, info = env.reset()\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# state, info = env.reset()\n",
    "# print (state.shape)\n",
    "# print (info)\n",
    "\n",
    "\"\"\"\n",
    "记录：\n",
    "1.第一个5000轮，loss没有收敛，render看了一下训练成果，发现agent会卡在右上角不动，推测可能是训练轮数太少了，\n",
    "到后面epsilon又变成0.1，导致探索性不足，q值趋于不好的稳定。\n",
    "2.第二次，使用了上面第4000轮的权重，调整了一下epsilon从0.75-0.1，调整了一下lr=1.5e-4，但是loss发散了，发散的很严重，\n",
    "  赶紧停掉了，重新设计一下参数。\n",
    "3.第三次：\n",
    "lr=0.9e-4， epsilon_start, epsilon_end, epsilon_decay = 0.7, 0.1, 6000000， num_episodes = 5000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c584eb5-a33d-462c-87a4-cbd76c695934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (210, 160, 3)\n",
      "info {'lives': 3, 'episode_frame_number': 0, 'frame_number': 0}\n",
      "obs (4, 84, 84)\n",
      "info {'lives': 3, 'episode_frame_number': 2, 'frame_number': 2}\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "from gymnasium.wrappers import FrameStackObservation\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "step1: 预处理环境。\n",
    "why: atari的obs是210*160*3，应该先处理\n",
    "how: 1.转灰度 2.压缩 3.堆叠最近的4帧画面(这样能同时展示s和a)\n",
    "\"\"\"\n",
    "#train\n",
    "# env = gym.make(\"ALE/Alien-v5\",frameskip=1)\n",
    "\n",
    "# render\n",
    "env = gym.make(\"ALE/Alien-v5\",frameskip=1, render_mode='human')\n",
    "\n",
    "#print\n",
    "obs, info = env.reset()\n",
    "print ('obs', obs.shape)\n",
    "print ('info', info)\n",
    "\n",
    "# AtaAtariPreprocessing函数详解：\n",
    "# - frame_skip=4 一个动作保持4帧 -> 1.可以减少计算量 2.让动作效果更加明显， 这里atari已经内置了\n",
    "# - grayscale_obs=True 转化为灰度图像\n",
    "# - scale_obs=True 把像素值从【0,255】-> 【0,1】\n",
    "env = AtariPreprocessing(env, grayscale_obs=True, scale_obs=True)\n",
    "\n",
    "# 表示每次obs由最近的4帧图形拼接而成\n",
    "# shape（4,210,160）\n",
    "env = FrameStackObservation(env, stack_size=4)\n",
    "\n",
    "# print \n",
    "obs, info = env.reset()\n",
    "print ('obs', obs.shape)\n",
    "print ('info', info)\n",
    "print (env.action_space.n)\n",
    "# print(env.spec)              # 查看环境配置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8cfad-5cf5-420d-8846-f35cb59b7ca7",
   "metadata": {},
   "source": [
    "### EnvSpec 逐项解读（重点说明含义与影响）\n",
    "\n",
    "- id='ALE/Alien-v5'\n",
    "环境标识：Atari 的 Alien（版本 v5）。\n",
    "\n",
    "- entry_point='ale_py.env:AtariEnv'\n",
    "真正创建底层环境的类是 ALE（Arcade Learning Environment）的 AtariEnv。\n",
    "\n",
    "- kwargs={...}（底层 env 的参数）\n",
    "\n",
    "- game: 'alien'：游戏名。\n",
    "\n",
    "- repeat_action_probability: 0.25：sticky actions（黏性动作）概率为 0.25，表示有 25% 概率重复上一个动作，增加环境随机性。\n",
    "\n",
    "- full_action_space: False：使用 minimal action set 而不是所有动作。\n",
    "\n",
    "- frameskip: 1：底层 ALE 本身的 frameskip=1（非常重要——不是最终跳帧的意思；wrapper 可能会再做跳帧）。\n",
    "\n",
    "- max_num_frames_per_episode: 108000：每个 episode 最多 108000 帧（Atari 的标准——约 30 分钟）。\n",
    "\n",
    "- render_mode: 'human'：渲染模式。\n",
    "\n",
    "- max_episode_steps=None\n",
    "表示没有被 TimeLimit（Gym 的 step 上限封装器）包裹，所以 env.spec 的 max_episode_steps 是 None。不过底层的 max_num_frames_per_episode 仍然存在（上面那项）。\n",
    "\n",
    "- additional_wrappers=(WrapperSpec(...), WrapperSpec(...))\n",
    "这非常关键 —— Gymnasium 在 make(\"ALE/Alien-v5\") 时自动为你套了两个 wrapper（你通常不需要再手动套一次）：\n",
    "\n",
    "**AtariPreprocessing 的 kwargs:**\n",
    "\n",
    "- noop_max=30：reset 时会随机做 0–30 个 NOOP，用于打乱起始状态。\n",
    "\n",
    "- frame_skip=4：AtariPreprocessing 会把每个动作重复执行 4 帧 —— 这就是常说的跳帧（把 60 FPS 降为 15 FPS 的效果）。\n",
    "\n",
    "- screen_size=84：会把屏幕缩到 84×84。\n",
    "\n",
    "- terminal_on_life_loss=False：失去一条命不会把 episode 标记为 terminated（很多实现可选这个行为）。\n",
    "\n",
    "- grayscale_obs=True：转灰度。\n",
    "\n",
    "- grayscale_newaxis=False：灰度不会被加成单独的最后轴（意味着单帧是 2D (H,W) 而不是 (H,W,1)）。\n",
    "\n",
    "- scale_obs=True：把像素归一化到 [0,1]（仍为 uint8、0–255）。\n",
    "\n",
    "**FrameStackObservation 的 kwargs:**\n",
    "\n",
    "- stack_size=4：堆叠最近 4 帧 → 最终 observation 包含 4 帧历史。\n",
    "\n",
    "- padding_type='reset'：在 episode 开始时，空的前帧用 reset 的观测填充（而不是用 0）。\n",
    "\n",
    "- 注意：顺序通常是先 AtariPreprocessing（做灰度/resize/跳帧），再 FrameStackObservation（在预处理后的帧上做堆叠）。\n",
    "\n",
    "- vector_entry_point='ale_py.vector_env:AtariVectorEnv'\n",
    "- 环境支持 vectorized（并行）版本，用于同时跑多个 env。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99957db7-65ca-482e-b79f-2d32ce9c2c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===test DQN===\n",
      "torch.Size([1, 4, 84, 84])\n",
      "torch.Size([1, 18])\n",
      "tensor([[ 0.0208, -0.0010,  0.0201, -0.0610, -0.0120,  0.0046, -0.0482, -0.0547,\n",
      "          0.0184, -0.0265,  0.0788,  0.0070,  0.0055, -0.0030, -0.0356,  0.0215,\n",
      "          0.0370,  0.0303]], grad_fn=<AddmmBackward0>)\n",
      "===test DQN===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "class DQN\n",
    "input : state s and action a\n",
    "        in other words, 4-frame obs (N, 4, 84, 84)\n",
    "return : q(s,a)\n",
    "\"\"\"\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4), ## output=(N, 32, 20, 20)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64, kernel_size=4, stride=2), ## output=(N, 64, 9, 9)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), ## output = (N, 64, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), ## 64*7*7 = 3136\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_dim) # 对应每个a的q值\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y\n",
    "\n",
    "\"\"\"\n",
    "test DQN\n",
    "input: a dummy tensor to test DQN\n",
    "\"\"\"\n",
    "def test_DQN():\n",
    "    print (\"===test DQN===\")\n",
    "    dqn = DQN(env.action_space.n)\n",
    "    dummy_x = torch.randn((1,4,84,84))\n",
    "    print (dummy_x.shape)\n",
    "    y = dqn(dummy_x)\n",
    "    print (y.shape)\n",
    "    print (y)\n",
    "    print (\"===test DQN===\\n\")\n",
    "\n",
    "test_DQN()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class ReplayBuffer\n",
    "why: 1.打破数据的事件关联性 2.增强样本的利用率 3.提高训练的稳定性\n",
    "how: 把经验放在deque容器buffer中， 需要的时候随机采样batch_size个\n",
    "\"\"\"\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity) # 双端队列deque, 如果满了， 最老的经验会被删除\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size) #从buffer中采样 batch_size 个样本\n",
    "\n",
    "        # *是解包操作unpacking, 这是把batch解包后， 按列组合\n",
    "        state, action, reward, next_state, done = map(np.array, zip(*batch)) \n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "关键参数赋值\n",
    "\"\"\"\n",
    "epsilon_start, epsilon_end, epsilon_decay = 0.6, 0.1, 7000000\n",
    "gamma = 0.99\n",
    "batch_size = 32 # 一次梯度下降用这么多数量的数据\n",
    "update_target = 5000 # 更新target的频率\n",
    "\n",
    "num_episodes = 5000\n",
    "returns = []\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def train()\n",
    "封装整个训练过程\n",
    "\"\"\"\n",
    "from torch.utils.tensorboard import SummaryWriter # 用 tensorboard 记录训练过程\n",
    "def train(env):\n",
    "    device = torch.device('cuda')\n",
    "    action_dim = env.action_space.n\n",
    "    global_step = 0\n",
    "    writer = SummaryWriter(log_dir='runs/dqn3')\n",
    "\n",
    "    #qnet and target_net\n",
    "    qnet = DQN(action_dim).to(device)\n",
    "    target_net = DQN(action_dim).to(device)\n",
    "\n",
    "    qnet.load_state_dict(torch.load(\"runs/dqn_ckpt_4000.pth\", map_location=device))\n",
    "    \n",
    "    # 将qnet 的参数传入 target_net\n",
    "    # why两个net ： 1.虽然两个都是估计q(s,a) \n",
    "    #               2.先固定target_net，对损失函数求偏导的时候就不会太复杂，然后再把更新后的参数赋给targetnet\n",
    "    #               3. 这样还可以稳定训练，减少 Q 值振荡\n",
    "    target_net.load_state_dict(qnet.state_dict())\n",
    "\n",
    "    # 创建实例\n",
    "    optimizer = optim.Adam(qnet.parameters(), lr=1.5e-4)\n",
    "    buffer = ReplayBuffer()\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state)\n",
    "        episode_loss = 0\n",
    "\n",
    "        # max = num_episodes * steps\n",
    "        # steps 最大为 108000 / 4, 根据 env.spec()查看得 'max_num_frames_per_episode': 108000\n",
    "        # 上面有设置 skip_frame = 4, \n",
    "        episode_reward = 0\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            # 用 epsilon-greeedy 策略采样episode， 刚开始多exploration, 后来多exploitation\n",
    "            # global_step = 0 -> epsilon = 1.0\n",
    "            # global_step = 正无穷 -> epsilon = 0.1\n",
    "            epsilon = max(epsilon_end, epsilon_start - global_step / epsilon_decay)\n",
    "            # epsilon = 0.1\n",
    "\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)  # (1, 4, 84, 84)\n",
    "                    q_values = qnet(s)\n",
    "                    action = q_values.argmax(1).item() # q_value (1,18) -> argmax(1)指定在action space 维度\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = np.array(next_state)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # 放入经验 buffer 中\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            #更新状态\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            global_step += 1\n",
    "\n",
    "\n",
    "            # 用经验回放来训练\n",
    "            if len(buffer) > 2*batch_size:\n",
    "                s, a, r, ns, d = buffer.sample(batch_size)\n",
    "\n",
    "                #放在gpu上\n",
    "                s = torch.tensor(s, dtype=torch.float32, device=device)  \n",
    "                ns = torch.tensor(ns, dtype=torch.float32, device=device)\n",
    "                a = torch.tensor(a, dtype=torch.long, device=device)     \n",
    "                r = torch.tensor(r, dtype=torch.float32, device=device)  \n",
    "                d = torch.tensor(d, dtype=torch.float32, device=device)\n",
    "\n",
    "                # gather(dim, idx): dim=1表示在行上， 选取idx列的数据\n",
    "                # a = [0, 2, 1],  a.unsqueeze(1) = [[0], [2], [1]]\n",
    "                # 本质是从每个 batch 中选出采样到 qvalue\n",
    "                q_values = qnet(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # 在计算 target_q 的时候要锁住梯度计算， 不然会很复杂\n",
    "                    max_next_q = target_net(ns).max(1)[0]\n",
    "                    target_q = r + gamma * (1 - d) * max_next_q\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                episode_loss += loss.item() # 用于记录整个episode的loss\n",
    "\n",
    "            if global_step % update_target == 0:\n",
    "                target_net.load_state_dict(qnet.state_dict())\n",
    "\n",
    "        # 记录训练过程\n",
    "        writer.add_scalar(\"Reward\", episode_reward, episode)\n",
    "        writer.add_scalar(\"Loss\", episode_loss, episode)\n",
    "        writer.add_scalar(\"Epsilon\", epsilon, episode)\n",
    "        \n",
    "        returns.append(episode_reward)\n",
    "        print (f\"Episode {episode}, Return {episode_reward}, Epsilon {epsilon:.3f}\")\n",
    "\n",
    "        # save 权重， 不然一个意外就白跑了半天\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            torch.save(qnet.state_dict(), f'runs/dqn_ckpt_{episode+1}.pth')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bcb2717-199a-43f8-a683-3bab6ab3bf00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2ca8964-97ad-402a-89e9-15a07788e747",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (1, 4, 84, 84)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m q_values \u001b[38;5;241m=\u001b[39m qnet(s)\n\u001b[1;32m---> 26\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mq_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     29\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_state)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "用render展示训练成果\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device('cuda')\n",
    "action_dim = env.action_space.n\n",
    "global_step = 0\n",
    "# writer = SummaryWriter(log_dir='runs/dqn2')\n",
    "\n",
    "#qnet and target_net\n",
    "qnet = DQN(action_dim).to(device)\n",
    "target_net = DQN(action_dim).to(device)\n",
    "qnet.load_state_dict(torch.load(\"runs/dqn4_ckpt_1500.pth\", map_location=device))\n",
    "target_net.load_state_dict(qnet.state_dict())\n",
    "\n",
    "num_episode = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = np.array(state)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)  # (1, 4, 84, 84)\n",
    "        q_values = qnet(s)\n",
    "        action = q_values.argmax(1).item()\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = np.array(next_state)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f030392-8b77-44f7-8884-87f7e8cbac6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cbe014-b26b-4507-b8e4-aa2a3871c2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
