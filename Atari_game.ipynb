{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4e2cc1c-8b6c-4b3c-841c-f94b29a7f525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test code '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import wandb\n",
    "wandb.login(key=\"390acfbb12bfaf0cc52b7a946e4db99a58ed88f3\")\n",
    "\n",
    "\"\"\" test code \"\"\"\n",
    "# env = gym.make(\"ALE/Alien-v5\", render_mode=\"human\")\n",
    "# observation, info = env.reset()\n",
    "\n",
    "# for _ in range(500):\n",
    "#     action = env.action_space.sample()  # 随机动作\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "#     if terminated or truncated:\n",
    "#         observation, info = env.reset()\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# state, info = env.reset()\n",
    "# print (state.shape)\n",
    "# print (info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c584eb5-a33d-462c-87a4-cbd76c695934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (210, 160, 3)\n",
      "info {'lives': 3, 'episode_frame_number': 0, 'frame_number': 0}\n",
      "obs (4, 84, 84)\n",
      "info {'lives': 3, 'episode_frame_number': 18, 'frame_number': 18}\n",
      "18\n",
      "EnvSpec(id='ALE/Alien-v5', entry_point='ale_py.env:AtariEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, disable_env_checker=False, kwargs={'game': 'alien', 'repeat_action_probability': 0.25, 'full_action_space': False, 'frameskip': 1, 'max_num_frames_per_episode': 108000, 'render_mode': 'human'}, namespace='ALE', name='Alien', version=5, additional_wrappers=(WrapperSpec(name='AtariPreprocessing', entry_point='gymnasium.wrappers.atari_preprocessing:AtariPreprocessing', kwargs={'noop_max': 30, 'frame_skip': 4, 'screen_size': 84, 'terminal_on_life_loss': False, 'grayscale_obs': True, 'grayscale_newaxis': False, 'scale_obs': True}), WrapperSpec(name='FrameStackObservation', entry_point='gymnasium.wrappers.stateful_observation:FrameStackObservation', kwargs={'stack_size': 4, 'padding_type': 'reset'})), vector_entry_point='ale_py.vector_env:AtariVectorEnv')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "from gymnasium.wrappers import FrameStackObservation\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "step1: 预处理环境。\n",
    "why: atari的obs是210*160*3，应该先处理\n",
    "how: 1.转灰度 2.压缩 3.堆叠最近的4帧画面(这样能同时展示s和a)\n",
    "\"\"\"\n",
    "\n",
    "env = gym.make(\"ALE/Alien-v5\",frameskip=1, render_mode=\"human\")\n",
    "\n",
    "#print\n",
    "obs, info = env.reset()\n",
    "print ('obs', obs.shape)\n",
    "print ('info', info)\n",
    "\n",
    "# AtaAtariPreprocessing函数详解：\n",
    "# - frame_skip=4 一个动作保持4帧 -> 1.可以减少计算量 2.让动作效果更加明显， 这里atari已经内置了\n",
    "# - grayscale_obs=True 转化为灰度图像\n",
    "# - scale_obs=True 把像素值从【0,255】-> 【0,1】\n",
    "env = AtariPreprocessing(env, grayscale_obs=True, scale_obs=True)\n",
    "\n",
    "# 表示每次obs由最近的4帧图形拼接而成\n",
    "# shape（4,210,160）\n",
    "env = FrameStackObservation(env, stack_size=4)\n",
    "\n",
    "# print \n",
    "obs, info = env.reset()\n",
    "print ('obs', obs.shape)\n",
    "print ('info', info)\n",
    "print (env.action_space.n)\n",
    "print(env.spec)              # 查看环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8cfad-5cf5-420d-8846-f35cb59b7ca7",
   "metadata": {},
   "source": [
    "### EnvSpec 逐项解读（重点说明含义与影响）\n",
    "\n",
    "- id='ALE/Alien-v5'\n",
    "环境标识：Atari 的 Alien（版本 v5）。\n",
    "\n",
    "- entry_point='ale_py.env:AtariEnv'\n",
    "真正创建底层环境的类是 ALE（Arcade Learning Environment）的 AtariEnv。\n",
    "\n",
    "- kwargs={...}（底层 env 的参数）\n",
    "\n",
    "- game: 'alien'：游戏名。\n",
    "\n",
    "- repeat_action_probability: 0.25：sticky actions（黏性动作）概率为 0.25，表示有 25% 概率重复上一个动作，增加环境随机性。\n",
    "\n",
    "- full_action_space: False：使用 minimal action set 而不是所有动作。\n",
    "\n",
    "- frameskip: 1：底层 ALE 本身的 frameskip=1（非常重要——不是最终跳帧的意思；wrapper 可能会再做跳帧）。\n",
    "\n",
    "- max_num_frames_per_episode: 108000：每个 episode 最多 108000 帧（Atari 的标准——约 30 分钟）。\n",
    "\n",
    "- render_mode: 'human'：渲染模式。\n",
    "\n",
    "- max_episode_steps=None\n",
    "表示没有被 TimeLimit（Gym 的 step 上限封装器）包裹，所以 env.spec 的 max_episode_steps 是 None。不过底层的 max_num_frames_per_episode 仍然存在（上面那项）。\n",
    "\n",
    "- additional_wrappers=(WrapperSpec(...), WrapperSpec(...))\n",
    "这非常关键 —— Gymnasium 在 make(\"ALE/Alien-v5\") 时自动为你套了两个 wrapper（你通常不需要再手动套一次）：\n",
    "\n",
    "**AtariPreprocessing 的 kwargs:**\n",
    "\n",
    "- noop_max=30：reset 时会随机做 0–30 个 NOOP，用于打乱起始状态。\n",
    "\n",
    "- frame_skip=4：AtariPreprocessing 会把每个动作重复执行 4 帧 —— 这就是常说的跳帧（把 60 FPS 降为 15 FPS 的效果）。\n",
    "\n",
    "- screen_size=84：会把屏幕缩到 84×84。\n",
    "\n",
    "- terminal_on_life_loss=False：失去一条命不会把 episode 标记为 terminated（很多实现可选这个行为）。\n",
    "\n",
    "- grayscale_obs=True：转灰度。\n",
    "\n",
    "- grayscale_newaxis=False：灰度不会被加成单独的最后轴（意味着单帧是 2D (H,W) 而不是 (H,W,1)）。\n",
    "\n",
    "- scale_obs=True：把像素归一化到 [0,1]（仍为 uint8、0–255）。\n",
    "\n",
    "**FrameStackObservation 的 kwargs:**\n",
    "\n",
    "- stack_size=4：堆叠最近 4 帧 → 最终 observation 包含 4 帧历史。\n",
    "\n",
    "- padding_type='reset'：在 episode 开始时，空的前帧用 reset 的观测填充（而不是用 0）。\n",
    "\n",
    "- 注意：顺序通常是先 AtariPreprocessing（做灰度/resize/跳帧），再 FrameStackObservation（在预处理后的帧上做堆叠）。\n",
    "\n",
    "- vector_entry_point='ale_py.vector_env:AtariVectorEnv'\n",
    "- 环境支持 vectorized（并行）版本，用于同时跑多个 env。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99957db7-65ca-482e-b79f-2d32ce9c2c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===test DQN===\n",
      "torch.Size([1, 4, 84, 84])\n",
      "torch.Size([1, 18])\n",
      "tensor([[ 0.0149,  0.0533,  0.0003, -0.0331,  0.0247,  0.0067, -0.0500, -0.0023,\n",
      "          0.0110, -0.0395,  0.0067,  0.0283,  0.0028, -0.0329,  0.0209,  0.0276,\n",
      "          0.0064, -0.0637]], grad_fn=<AddmmBackward0>)\n",
      "===test DQN===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "class DQN\n",
    "input : state s and action a\n",
    "        in other words, 4-frame obs (N, 4, 84, 84)\n",
    "return : q(s,a)\n",
    "\"\"\"\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4), ## output=(N, 32, 20, 20)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64, kernel_size=4, stride=2), ## output=(N, 64, 9, 9)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), ## output = (N, 64, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), ## 64*7*7 = 3136\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_dim) # 对应每个a的q值\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y\n",
    "\n",
    "\"\"\"\n",
    "test DQN\n",
    "input: a dummy tensor to test DQN\n",
    "\"\"\"\n",
    "def test_DQN():\n",
    "    print (\"===test DQN===\")\n",
    "    dqn = DQN(env.action_space.n)\n",
    "    dummy_x = torch.randn((1,4,84,84))\n",
    "    print (dummy_x.shape)\n",
    "    y = dqn(dummy_x)\n",
    "    print (y.shape)\n",
    "    print (y)\n",
    "    print (\"===test DQN===\\n\")\n",
    "\n",
    "test_DQN()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class ReplayBuffer\n",
    "why: 1.打破数据的事件关联性 2.增强样本的利用率 3.提高训练的稳定性\n",
    "how: 把经验放在deque容器buffer中， 需要的时候随机采样batch_size个\n",
    "\"\"\"\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity) # 双端队列deque, 如果满了， 最老的经验会被删除\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size) #从buffer中采样 batch_size 个样本\n",
    "\n",
    "        # *是解包操作unpacking, 这是把batch解包后， 按列组合\n",
    "        state, action, reward, next_state, done = map(np.array, zip(*batch)) \n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "关键参数赋值\n",
    "\"\"\"\n",
    "epsilon_start, epsilon_end, epsilon_decay = 1.0, 0.1, 100000\n",
    "gamma = 0.98\n",
    "batch_size = 32 # 一次梯度下降用这么多数量的数据\n",
    "update_target = 5000 # 更新target的频率\n",
    "\n",
    "num_episodes = 5000\n",
    "returns = []\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def train()\n",
    "封装整个训练过程\n",
    "\"\"\"\n",
    "def train(env):\n",
    "    device = torch.device('cuda')\n",
    "    action_dim = env.action_space.n\n",
    "    global_step = 0\n",
    "\n",
    "    #qnet and target_net\n",
    "    qnet = DQN(action_dim).to(device)\n",
    "    target_net = DQN(action_dim).to(device)\n",
    "\n",
    "    # 将qnet 的参数传入 target_net\n",
    "    # why两个net ： 1.虽然两个都是估计q(s,a) \n",
    "    #               2.先固定target_net，对损失函数求偏导的时候就不会太复杂，然后再把更新后的参数赋给targetnet\n",
    "    #               3. 这样还可以稳定训练，减少 Q 值振荡\n",
    "    target_net.load_state_dict(qnet.state_dict())\n",
    "\n",
    "    # 创建实例\n",
    "    optimizer = optim.Adam(qnet.parameters(), lr=1e-4)\n",
    "    buffer = ReplayBuffer()\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state)\n",
    "\n",
    "        # max = num_episodes * steps\n",
    "        # steps 最大为 108000 / 4, 根据 env.spec()查看得 'max_num_frames_per_episode': 108000\n",
    "        # 上面有设置 skip_frame = 4, \n",
    "        episode_reward = 0\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            # 用 epsilon-greeedy 策略采样episode， 刚开始多exploration, 后来多exploitation\n",
    "            # global_step = 0 -> epsilon = 1.0\n",
    "            # global_step = 正无穷 -> epsilon = 0.1\n",
    "            epsilon = epsilon_end + (epsilon_start- epsilon_end) * np.exp(-global_step / epsilon_decay)\n",
    "\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)  # (1, 4, 84, 84)\n",
    "                    q_values = qnet(s)\n",
    "                    action = q_values.argmax(1).item() # q_value (1,18) -> argmax(1)指定在action space 维度\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = np.array(next_state)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # 放入经验 buffer 中\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            #更新状态\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            global_step += 1\n",
    "\n",
    "\n",
    "            # 用经验回放来训练\n",
    "            if len(buffer) > batch_size:\n",
    "                s, a, r, ns, d = buffer.sample(batch_size)\n",
    "\n",
    "                #放在gpu上\n",
    "                s = torch.tensor(s, dtype=torch.float32, device=device)  \n",
    "                ns = torch.tensor(ns, dtype=torch.float32, device=device)\n",
    "                a = torch.tensor(a, dtype=torch.long, device=device)     \n",
    "                r = torch.tensor(r, dtype=torch.float32, device=device)  \n",
    "                d = torch.tensor(d, dtype=torch.float32, device=device)\n",
    "\n",
    "                # gather(dim, idx): dim=1表示在行上， 选取idx列的数据\n",
    "                # a = [0, 2, 1],  a.unsqueeze(1) = [[0], [2], [1]]\n",
    "                # 本质是从每个 batch 中选出采样到 qvalue\n",
    "                q_values = qnet(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # 在计算 target_q 的时候要锁住梯度计算， 不然会很复杂\n",
    "                    max_next_q = target_net(ns).max(1)[0]\n",
    "                    target_q = r + gamma * (1 - d) * max_next_q\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if global_step % update_target == 0:\n",
    "                target_net.load_state_dict(qnet.state_dict())\n",
    "\n",
    "        returns.append(episode_reward)\n",
    "        print (f\"Episode {episode}, Return {episode_reward}, Epsilon {epsilon:.3f}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bcb2717-199a-43f8-a683-3bab6ab3bf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 1/5000 [00:42<58:33:01, 42.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Return 130.0, Epsilon 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 2/5000 [01:47<77:41:05, 55.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Return 200.0, Epsilon 0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 3/5000 [02:33<71:24:19, 51.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2, Return 150.0, Epsilon 0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 4/5000 [04:30<107:09:32, 77.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3, Return 260.0, Epsilon 0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 5/5000 [05:26<96:15:24, 69.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4, Return 200.0, Epsilon 0.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 6/5000 [06:19<88:49:21, 64.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5, Return 160.0, Epsilon 0.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 7/5000 [06:41<69:51:28, 50.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6, Return 140.0, Epsilon 0.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 8/5000 [07:02<56:50:48, 41.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7, Return 170.0, Epsilon 0.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 9/5000 [07:30<51:12:28, 36.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8, Return 180.0, Epsilon 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 10/5000 [09:03<74:55:05, 54.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9, Return 230.0, Epsilon 0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 11/5000 [10:13<81:36:19, 58.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Return 120.0, Epsilon 0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 12/5000 [11:33<90:38:57, 65.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11, Return 190.0, Epsilon 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                      | 13/5000 [13:03<101:00:25, 72.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12, Return 120.0, Epsilon 0.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                      | 14/5000 [14:35<108:54:12, 78.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13, Return 180.0, Epsilon 0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                      | 15/5000 [16:32<124:56:55, 90.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14, Return 220.0, Epsilon 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                      | 16/5000 [18:11<128:39:09, 92.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15, Return 100.0, Epsilon 0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                    | 17/5000 [20:10<139:16:47, 100.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16, Return 250.0, Epsilon 0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                     | 18/5000 [21:30<130:53:27, 94.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17, Return 190.0, Epsilon 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                     | 19/5000 [22:35<118:36:02, 85.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18, Return 120.0, Epsilon 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                     | 20/5000 [24:21<126:55:15, 91.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19, Return 210.0, Epsilon 0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                     | 20/5000 [25:38<106:24:29, 76.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 129\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m    126\u001b[0m         q_values \u001b[38;5;241m=\u001b[39m qnet(s)\n\u001b[0;32m    127\u001b[0m         action \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# q_value (1,18) -> argmax(1)指定在action space 维度\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_state)\n\u001b[0;32m    131\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mE:\\anacoda\\envs\\torch310\\lib\\site-packages\\gymnasium\\wrappers\\stateful_observation.py:425\u001b[0m, in \u001b[0;36mFrameStackObservation.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and info from the environment\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m    428\u001b[0m     updated_obs \u001b[38;5;241m=\u001b[39m deepcopy(\n\u001b[0;32m    429\u001b[0m         concatenate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs)\n\u001b[0;32m    430\u001b[0m     )\n",
      "File \u001b[1;32mE:\\anacoda\\envs\\torch310\\lib\\site-packages\\gymnasium\\wrappers\\atari_preprocessing.py:163\u001b[0m, in \u001b[0;36mAtariPreprocessing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    160\u001b[0m total_reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_skip):\n\u001b[1;32m--> 163\u001b[0m     _, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_over \u001b[38;5;241m=\u001b[39m terminated\n",
      "File \u001b[1;32mE:\\anacoda\\envs\\torch310\\lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anacoda\\envs\\torch310\\lib\\site-packages\\gymnasium\\core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anacoda\\envs\\torch310\\lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anacoda\\envs\\torch310\\lib\\site-packages\\ale_py\\env.py:305\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    303\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[1;32m--> 305\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mact(action_idx, strength)\n\u001b[0;32m    307\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    308\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"dqn-atari\",       \n",
    "    entity=\"Mingyu Liu\",    \n",
    "    config={\n",
    "        \"env\": \"ALE/Alien-v5\",\n",
    "        \"batch_size\": 32,\n",
    "        \"gamma\": 0.99,\n",
    "        \"lr\": 1e-4,\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.1\n",
    "    }\n",
    ")\n",
    "config = wandb.config\n",
    "\n",
    "\n",
    "train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca8964-97ad-402a-89e9-15a07788e747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
